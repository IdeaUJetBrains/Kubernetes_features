chain:
  numberOfNodes: 1
subchart1:
  enabled: true


# Default values for anchore_engine chart.

# The configuration for the API service, which must be reachable inside the cluster by other workers and users
service:
  type: ClusterIP
  ports:
    api: 8228
    queue: 8083
    catalog: 8082
    policy: 8087
    k8sImagePolicyWebhook: 8338

image:
  # Can use 'latest' but not recommended
  tag: docker.io/anchore/anchore-engine:v0.1.10
  # pullPolicy: IfNotPresent

# Used to create Ingress record (should used with service.type: ClusterIP or NodePort depending on platform)
ingress:
  enabled: false
  annotations:
  # kubernetes.io/ingress.allow-http: False
  # kubernetes.io/ingress.class: nginx
  # kubernetes.io/tls-acme: true

  # Secrets must be manually created in the namespace.
  tls:
  # - secretName: tlstestsecret

# Dependency on Postgresql, configure here
postgresql:
  enabled: true
  postgresUser: anchoreengine
  postgresPassword: anchore-postgres,123
  postgresDatabase: anchore


  # Use this config if you set enabled=False and want to specify an external (already existing) postres deployment for use.
  # Set this to the host and port. eg. mypostgres.myserver.io:5432
  externalEndpoint: Null


# Global configuration shared by both core and worker
globalConfig:
  # Set where default configs are placed at startup. This must be a writable location for the pod.
  configDir: /anchore_service_config

  dbConfig:
    timeout: 120
    # Use ssl, but the default postgresql config in helm's stable repo does not support ssl on server side, so this should be set for external dbs only for the time being
    ssl: false
    connectionPoolSize: 30
    connectionPoolMaxOverflow: 100

  # Cleanup local images used during analysis, defaults to True. If set to false, images will remain on workers after analysis.
  cleanupImages: true

  # If True, if a user adds an ECR registry with username = awsauto then the system will look for an instance profile to use for auth against the registry
  allowECRUseIAMRole: false

  # User configuration. Add more users here if needed.
  users:
    admin:
      password: foobar
      email: admin@myemail.com
      policyBundleSyncEnabled: false

      # Credentials for https://anchore.io Cloud service if you have them. Can be used to automatically sync policy bundles.
      anchoreIOCredentials:
        # If use_anonymous = False, the specific credentials are used for the feed sync and bundle sync features otherwise disregarded.
        useAnonymous: true
        user: someuser
        password: somepassword

  internalServicesSslEnabled: false
  internalServicesSslVerifyCerts: false

  # Intervals to run specific events on (seconds)
  cycleTimers:
    # Interval to check for an update to a tag
    image_watcher: 3600
    # Interval to re-run a policy eval on a tag
    policy_eval: 3600
    # Interval to run a feed sync to get latest cve data
    feed_sync: 14400
    # Interval workers check the queue
    analyzer_queue: 1
    # Interval notifications will be processed for state changes
    notifications: 30
    # Intervals service state updates are polled
    service_watcher: 15
    # Interval for policy bundle sync from anchore.io if enabled
    policy_bundle_sync: 300


# Configuration for the core engine service that serves the API
# The core service handles the user facing APIs and coordination of workers as well as storage interfaces for data
coreConfig:
  # For the moment, the replica count should stay at 1. That restriction should change soon.
  replicaCount: 1
  hostId: "anchore-engine-core-host"
  logLevel: INFO
  policyBundleSyncEnabled: false

  ssl:
    # To use certs for TLS directly from the services, create a secret with keys that match the values fo certSecretKey and certSecretCert
    certSecret: null
    certSecretKeyName: "tls.key"
    certSecretCertName: "tls.crt"
    certDir: "/certs"


  # Configure webhook outputs here. The service provides these webhooks for notifying external systems of updates
  webhooks:
    enabled: "True"
    config:
      # User and password to be set (using HTTP basic auth) on all webhook calls if necessary
      user: null
      password: null
      ssl_verify: true

      # Endpoint for general notification delivery. These events are image/tag updates etc. This is globally configured
      # and updates for all users are sent to the same host but with a different path for each user.
      general: {}
      # url: "http://somehost:9090/<notification_type>/<userId>"
      # Endpoint and credentials for policy evaluation delivery
      policy_eval: {}
        # url: "http://somehost:9090/policy_eval/<userId>"
        # user: null
      # password: null
      # Endpoint for fatal system errors to be delivery
      error_event: {}
      # url: 'http://somehost:9090/error_event/'

  # resources:
  #  limits:
  #    cpu: 100m
  #    memory: 6Gi
  #  requests:
  #    cpu: 100m
  #    memory: 4Gi

  ## Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  tolerations: []

  affinity: {}

# Configuration for the worker pods that perform image analysis
# There may be many of these workers but best practice is to not have more than one per node since analysis
# is very IO intensive. Use of affinity/anti-affinity rules for scheduling the workers is future work.
workerConfig:
  replicaCount: 1
  logLevel: INFO

  # Analyzer driver determines how the worker performs download and extraction of images
  #
  # "nodocker" does not require docker and pulls content directly from the registry and assembles it directly. This has the side-effect
  #    of not handling images with V1 manifests as cleanly due to missing metadata in those manifests.
  #
  # "localanchore" requires the docker socket mounted into the worker container and uses docker itself to pull and extract the image into the container.
  #    this is the legacy mode.
  analyzerMode: nodocker

  # The cycle timer is the interval between checks to the work queue for new jobs
  cycleTimerSeconds: 1

  # Controls the concurrency of the worker itself. Can be configured to process more than one task at a time, but it IO bound, so may not
  # necessarily be faster depending on hardware. Should test and balance this value vs. number of workers for your deployment cluster performance.
  concurrentTasksPerWorker: 1

  # The analysisVolume controls the mounting of an external volume for scratch space for image analysis. Generally speaking
  # you need to provision 3x the size of the largest image (uncompressed) that you want to analyze for this space.
  analysisScratchVolume:
    mountPath: /tmp
    details:
      emptyDir: {}

  # Configuration for ssl used for internal node communications between components
  ssl:
    certDir: "/certs"
    certSecret: null
    certSecretKeyName: "tls.key"
    certSecretCertName: "tls.crt"

  # resources:
  #  limits:
  #    cpu: 100m
  #    memory: 3Gi
  #  requests:
  #    cpu: 100m
  #    memory: 2Gi

  ## Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  tolerations: []

  affinity: {}


## postgres image repository
image: "postgres"
## postgres image version
## ref: https://hub.docker.com/r/library/postgres/tags/
##
imageTag: "9.6.2"

## Specify a imagePullPolicy
## 'Always' if imageTag is 'latest', else set to 'IfNotPresent'
## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
##
# imagePullPolicy:

## Specify imagePullSecrets
## ref: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
##
# imagePullSecrets: myregistrykey

## Create a database user
## Default: postgres
# postgresUser:
## Default: random 10 character string
# postgresPassword:

## Inject postgresPassword via a volume mount instead of environment variable
usePasswordFile: false

## Use Existing secret instead of creating one
## It must have a postgres-password key containing the desired password
# existingSecret: 'secret'

## Create a database
## Default: the postgres user
# postgresDatabase:

## Specify initdb arguments, e.g. --data-checksums
## ref: https://github.com/docker-library/docs/blob/master/postgres/content.md#postgres_initdb_args
## ref: https://www.postgresql.org/docs/current/static/app-initdb.html
# postgresInitdbArgs:

## Use an alternate scheduler, e.g. "stork".
## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
##
# schedulerName:

## Specify runtime config parameters as a dict, using camelCase, e.g.
## {"sharedBuffers": "500MB"}
## ref: https://www.postgresql.org/docs/current/static/runtime-config.html
# postgresConfig:

## Persist data to a persistent volume
persistence:
  enabled: true

  ## A manually managed Persistent Volume and Claim
  ## Requires persistence.enabled: true
  ## If defined, PVC must be created manually before volume will be bound
  # existingClaim:

  ## database data Persistent Volume Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  # storageClass: "-"
  accessMode: ReadWriteOnce
  size: 8Gi
  subPath: "postgresql-db"
  mountPath: /var/lib/postgresql/data/pgdata

  # annotations: {}

metrics:
  enabled: false
  image: wrouesnel/postgres_exporter
  imageTag: v0.1.1
  imagePullPolicy: IfNotPresent
  resources:
    requests:
      memory: 256Mi
      cpu: 100m
    ## Define additional custom metrics
    ## ref: https://github.com/wrouesnel/postgres_exporter#adding-new-metrics-via-a-config-file
    # customMetrics:
    #   pg_database:
    #     query: "SELECT d.datname AS name, CASE WHEN pg_catalog.has_database_privilege(d.datname, 'CONNECT') THEN pg_catalog.pg_database_size(d.datname) ELSE 0 END AS size FROM pg_catalog.pg_database d where datname not in ('template0', 'template1', 'postgres')"
    #     metrics:
    #       - name:
    #           usage: "LABEL"
    #           description: "Name of the database"
    #       - size_bytes:
    #           usage: "GAUGE"
    #           description: "Size of the database in bytes"

## Configure resource requests and limits
## ref: http://kubernetes.io/docs/user-guide/compute-resources/
##
resources:
  requests:
    memory: 256Mi
    cpu: 100m

service1:
  type: ClusterIP
  port: 5432
  externalIPs: []
  ## Manually set NodePort value
  ## Requires service.type: NodePort
  # nodePort:

networkPolicy:
  ## Enable creation of NetworkPolicy resources.
  ##
  enabled: false

  ## The Policy model to apply. When set to false, only pods with the correct
  ## client label will have network access to the port PostgreSQL is listening
  ## on. When true, PostgreSQL will accept connections from any source
  ## (with the correct destination port).
  ##
  allowExternal: true

## Node labels and tolerations for pod assignment
## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#taints-and-tolerations-beta-feature
nodeSelector: {}
tolerations: []
affinity: {}

# Override default liveness & readiness probes
probes:
  liveness:
    initialDelay: 60
    timeoutSeconds: 5
    failureThreshold: 6
  readiness:
    initialDelay: 5
    timeoutSeconds: 3
    periodSeconds: 5
## Annotations for the deployment and nodes.
deploymentAnnotations: {}
podAnnotations: {}

